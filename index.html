<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Providing a New Benchmark for evaluating Vision-Language Models for Advanced Map Queries">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MAPWise: New BenchMark for Advanced Map based Queries</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>


<style>
  .carousel-wrapper {
    position: relative;
    max-width: 600px;
    margin: auto;
    overflow: hidden;
  }
  .carousel {
    display: flex;
    transition: transform 0.5s ease-in-out;
  }
  .carousel-item {
    min-width: 100%;
    display: none;
  }
  .carousel-item.active {
    display: block;
  }
  .carousel-control {
    position: absolute;
    top: 50%;
    transform: translateY(-50%);
    background: rgba(0, 0, 0, 0.5);
    color: white;
    border: none;
    padding: 10px;
    cursor: pointer;
    font-size: 18px;
  }
  .prev {
    left: 10px;
  }
  .next {
    right: 10px;
  }
  </style>
</head>

<body>


  <section class="hero custom-hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MAPWise: Evaluating Vision-Language Models for Advanced Map Queries
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="srija.mukhopadhyay@research.iiit.ac.in">Srija Mukhopadhyay</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://abhishekrajgaria.github.io/">Abhishek Rajgaria</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://preranakh.github.io/">Prerana Khatiwada</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.iiit.ac.in/people/faculty/m.shrivastava/">Manish Shrivastava</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cis.upenn.edu/~danroth/">Dan Roth</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://vgupta123.github.io/">Vivek Gupta</a><sup>5</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>IIIT Hyderabad,</span>
              <span class="author-block"><sup>2</sup>University of Utah,</span>
              <span class="author-block"><sup>3</sup>University of Delaware,</span>
              <span class="author-block"><sup>4</sup>University of Pennsylvania,</span>
              <span class="author-block"><sup>5</sup>Arizona State University,</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2409.00255" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2409.00255" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. Will be edited with new Repo Link -->
                <span class="link-block">
                  <a href="https://github.com/abhishekrajgaria/Mapwise"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/abhishekrajgaria/Mapwise/tree/main/dataset"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-map"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- Explore Dataset-->
                <span class="link-block">
                  <a href="explore.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-eye"></i>
                    </span>
                    <span>Explore</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- About Section -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">About</h2>
          <div class="content has-text-justified">
            <p>
              <strong>MAPWise: Evaluating Vision-Language Models for Advanced Map Queries</strong>
            </p>
            <p>
              MAPWise is a novel benchmark designed to evaluate the performance of Vision-Language Models (VLMs) in
              answering map-based questions. Choropleth maps, commonly used for geographical data representation, pose
              unique challenges for VLMs due to their reliance on color variations to encode information. Our study
              introduces a dataset featuring maps from the United States, India, and China, with 1000 manually created
              question-answer pairs per region.
              The dataset includes a diverse set of question templates to test spatial reasoning, numerical
              comprehension, and pattern recognition. Our goal is to push the boundaries of VLM capabilities in handling
              complex map-based reasoning tasks.
            </p>
            <p>
              <strong>TL;DR:</strong> MAPWise is a benchmark designed to evaluate Vision-Language Models (VLMs) on
              map-based questions. It includes 3,000 manually curated Q&A pairs across maps from the United States,
              India, and China, testing spatial reasoning, numerical comprehension, and pattern recognition.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Dataset Creation Procedure Section -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">MAPWise Dataset Creation Procedure</h2>

          <!-- Data Sources -->
          <h3 class="title is-4 has-text-left">Data Sources</h3>
          <div class="columns">
            <div class="column">
              <article class="media box">

                <div class="media-content">
                  <p><strong>India</strong></p>
                  <p>Data from the Reserve Bank of India's <em>Handbook of Statistics on Indian States</em>.</p>
                </div>
              </article>
            </div>
            <div class="column">
              <article class="media box">

                <div class="media-content">
                  <p><strong>USA</strong></p>
                  <p>Healthcare statistics from the Kaiser Family Foundation.</p>
                </div>
              </article>
            </div>
            <div class="column">
              <article class="media box">

                <div class="media-content">
                  <p><strong>China</strong></p>
                  <p>Socioeconomic data from the National Bureau of Statistics of China.</p>
                </div>
              </article>
            </div>
          </div>

          <!-- Map Variations -->
          <h3 class="title is-4 has-text-left">Map Variations</h3>
          <div class="columns">
            <div class="column">
              <div class="box has-text-centered">
                <h4 class="title is-5">Discrete Maps</h4>
                <p>Defined categorical legend with clear boundaries.</p>
              </div>
            </div>
            <div class="column">
              <div class="box has-text-centered">
                <h4 class="title is-5">Continuous Maps</h4>
                <p>Gradual spectrum-based legends.</p>
              </div>
            </div>
            <div class="column">
              <div class="box has-text-centered">
                <h4 class="title is-5">Annotated Maps</h4>
                <p>Maps with labels to aid interpretation.</p>
              </div>
            </div>
            <div class="column">
              <div class="box has-text-centered">
                <h4 class="title is-5">Hatched Maps</h4>
                <p>Alternative representations using texture.</p>
              </div>
            </div>
          </div>


          <!-- Question Generation & Validation Process Section -->
          <h3 class="title is-4 has-text-left">Question Generation & Validation</h3>
          <div class="content has-text-justified">
            <p>
              <strong></strong>To design a Robust Benchmark for Map-Based QA</strong>, we developed question templates
              that vary in difficulty, from simple <em>yes/no</em> questions to complex <em>region association</em>
              tasks requiring spatial reasoning. The dataset includes three primary question types:
              <strong>Binary questions</strong> that require a yes or no answer based on map features,
              <strong>Direct Value Extraction</strong> tasks that involve retrieving specific numerical or categorical
              values from maps, and
              <strong>Region Association</strong> questions that challenge models to identify or count regions meeting
              particular criteria, often requiring geospatial reasoning.
            </p>
            <p>
              To ensure high dataset quality, expert annotators meticulously reviewed all questions and answers. A
              rigorous benchmarking process, including human evaluation, was conducted to assess dataset reliability.
              This evaluation highlights the limitations of existing Vision-Language Models (VLMs) in handling complex
              map-based reasoning tasks and provides valuable insights for future improvements.
            </p>
          </div>



          <!-- Table: Example Questions and Answer Types -->
          <h3 class="title is-4 has-text-left">Example Questions and Answer Types</h3>
          <div class="table-container">
            <table class="table is-striped is-fullwidth">
              <thead>
                <tr>
                  <th>Answer Type</th>
                  <th>Example Question</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Binary</td>
                  <td>Yes or no: California is an outlier compared to its neighbors?</td>
                </tr>
                <tr>
                  <td>Single Word</td>
                  <td>Name the eastern-most state that belongs to a higher value range compared to all its neighbors.
                  </td>
                </tr>
                <tr>
                  <td>List</td>
                  <td>Which states in the East China Sea region have a value higher than state Guangdong?</td>
                </tr>
                <tr>
                  <td>Range</td>
                  <td>What is the least value range in the west coast region?</td>
                </tr>
                <tr>
                  <td>Count</td>
                  <td>How many states bordering Canada have a value lower than New Mexico?</td>
                </tr>
                <tr>
                  <td>Ranking</td>
                  <td>Rank Rajasthan, Gujarat, and Jammu & Kashmir in terms of the legend value in the region bordering
                    Pakistan.</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h3 class="title is-4 has-text-left">Dataset Statistics</h3>
          <div class="columns is-centered">
            <div class="column is-half">
              <figure class="image">
                <img src="assets/tables/map_wise_data_stats.png" alt="MAPWise Data Statistics">
                <figcaption class="has-text-centered"><em>MAPWise Dataset statistic</em></figcaption>
              </figure>
            </div>
          </div>





        </div>
      </div>
    </div>
  </section>

  

  <!-- Results & Analysis Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Results & Analysis</h2>

          <!-- Models Used -->
          <h3 class="title is-4 has-text-left">Models Evaluated</h3>
          <div class="content has-text-justified">
            <p>
              We evaluated multiple Vision-Language Models (VLMs) on the <strong>MAPWise</strong> dataset. The models
              included both <strong>closed-source</strong> and <strong>open-source</strong> multimodal models:
            </p>
            <ul>
              <li><strong>Closed-Source Models:</strong> GPT-4o, Gemini 1.5 Flash</li>
              <li><strong>Open-Source Models:</strong> CogAgent, InternLM-XComposer2, Idefics 2, Qwen VL</li>
            </ul>

          <h3 class="title is-4 has-text-left">In Depth Analysis</h3>
            <p>
              We conducted thorough Analysis to evaluate <strong>closed-source model performance</strong>, the
              <strong>effectiveness of different prompting strategies</strong>, and <strong>biases in model
                predictions</strong>. The study examined how models performed across different map types
              (<strong>discrete vs. continuous</strong>, <strong>colored vs. hatched</strong>), annotations
              (<strong>with vs. without labels</strong>), and <strong>country-wise variations</strong>. Additionally,
              model accuracy was compared across various <strong>question-answer types</strong>, revealing significant
              gaps in spatial and numerical reasoning. For Complete Results, Please visit Appendix in <a href="https://arxiv.org/pdf/2409.00255">Paper</a>
            </p>

            <div class="results-carousel">
              <div class="results-card active">
                <h4>MAPWise: A Challenging Benchmark</h4>
                <p>The MAPWise dataset reveals a significant performance gap between current Vision-Language Models (VLMs) and human performance, especially on complex reasoning tasks (e.g., counting, listing). Existing VLMs have substantial limitations in reasoning abilities, highlighting the need for further research. The average gap is close to 50% on some task types.</p>
              </div>
              
              <div class="results-card">
                  <h4>Model Performance Comparison</h4>
                  <p>GPT-4o consistently outperforms other models, followed by Gemini 1.5 Flash. Gemini excels with hatched maps due to better legend resolution, while GPT-4o is generally better due to stronger reasoning. Open-source models (Idefics2, InternLM) show promise but struggle with complex reasoning, particularly evident in QwenVL's low counting accuracy (4.26%). Both strong data extraction and sophisticated reasoning are crucial for geo-spatial understanding.</p>
              </div>
              
              <div class="results-card">
                  <h4>Prompt Effectiveness</h4>
                  <p>Standard Chain-of-Thought (COT) prompting generally outperforms Explicit Extraction and Reasoning (EER), except for Gemini 1.5 Flash, which performs similarly with both. Gemini 1.5 Flash has strong instruction-following capabilities. Larger models implicitly use an EER-like approach, showing progress in reasoning. Smaller models struggle with EER's complexity.</p>
              </div>
              
              <div class="results-card">
                  <h4>Discrete vs. Continuous Maps</h4>
                  <p>Models generally perform better on discrete maps than continuous maps, especially for counting and range extraction. Continuous maps pose challenges in legend range and color resolution. Models do better on single-word answers within the continuous category, possibly due to question simplicity.</p>
              </div>
              
              <div class="results-card">
                  <h4>Colored Maps vs. Hatched Maps</h4>
                  <p>Models consistently perform better on colored maps than hatched maps, indicating a preference for colored data representation. Even GPT-4o shows significant performance drops on hatched maps. Idefics2 is the most robust to this map type.</p>
              </div>
              
              <div class="results-card">
                  <h4>Maps with and without annotations</h4>
                  <p>Models perform similarly on maps with and without annotations. Sometimes, performance is *better* without annotations. Annotations are not a crucial factor for model performance in map understanding, though they can sometimes be beneficial.</p>
              </div>
              
              <div class="results-card">
                  <h4>Country-Wise Performance</h4>
                  <p>Open-source models show consistent performance across countries (USA, India, China), while closed-source models exhibit greater variation. Potential training data biases may contribute to performance differences in closed-source models across different geographic regions.</p>
              </div>
              
              <div class="results-card">
                  <h4>Analysis across Question and Answer Types</h4>
                  <p>Models excel at binary questions, followed by single-word answers (strong data extraction). Closed-source models do well on range questions. Counting and listing tasks are the most difficult, requiring complex reasoning, external knowledge, and geospatial understanding. These are challenging even for humans. Models struggle with most questions concerning relative regions, except for binary questions.</p>
              </div>
              
              <div class="results-card">
                  <h4>Comparison with Random Baselines</h4>
                  <p>Some models (CogAgent, QwenVL) perform below random baselines on binary questions and list precision. This is due to irrelevant responses, repeated tokens, failure to generate valid responses, poor data processing, hallucination, and a lack of task comprehension. Examples show failures in value deduction and task understanding.</p>
              </div>
            
              <div class="results-controls">
                <button class="results-control-btn" onclick="moveResultsCard(-1)">&#10094;</button>
                <button class="results-control-btn" onclick="moveResultsCard(1)">&#10095;</button>
              </div>
            
              <div class="results-indicators">
                <span class="indicator active" onclick="goToCard(0)"></span>
                <span class="indicator" onclick="goToCard(1)"></span>
                <span class="indicator" onclick="goToCard(2)"></span>
                <span class="indicator" onclick="goToCard(3)"></span>
                <span class="indicator" onclick="goToCard(4)"></span>
                <span class="indicator" onclick="goToCard(5)"></span>
                <span class="indicator" onclick="goToCard(6)"></span>
                <span class="indicator" onclick="goToCard(7)"></span>
                <span class="indicator" onclick="goToCard(8)"></span>
              </div>
            </div>
  
            <script>
            let currentResultsCard = 0;
            const resultsCards = document.querySelectorAll(".results-card");
            const indicators = document.querySelectorAll(".indicator");
            
            function moveResultsCard(direction) {
              resultsCards[currentResultsCard].classList.remove("active");
              indicators[currentResultsCard].classList.remove("active");
              
              currentResultsCard = (currentResultsCard + direction + resultsCards.length) % resultsCards.length;
              
              resultsCards[currentResultsCard].classList.add("active");
              indicators[currentResultsCard].classList.add("active");
            }
            
            function goToCard(index) {
              resultsCards[currentResultsCard].classList.remove("active");
              indicators[currentResultsCard].classList.remove("active");
              
              currentResultsCard = index;
              
              resultsCards[currentResultsCard].classList.add("active");
              indicators[currentResultsCard].classList.add("active");
            }
            
            // Auto-advance carousel every 5 seconds
            // setInterval(() => moveResultsCard(1), 5000);
            </script>
          </div>

          <!-- Carousel for Results -->
          <h3 class="title is-4 has-text-left">Performance Tables</h3>
          <div class="carousel-wrapper">
            <div class="carousel" id="imageCarousel">
              <div class="carousel-item active">
                <figure class="image">
                  <img src="assets/tables/closed_models.png" alt="Closed Models">
                  <figcaption class="has-text-centered"><em>Figure 1: Closed Model Performance</em></figcaption>
                </figure>
              </div>
              <div class="carousel-item">
                <figure class="image">
                  <img src="assets/tables/prompt_effectiveness.png" alt="Effectiveness of Prompt">
                  <figcaption class="has-text-centered"><em>Figure 2: Effectiveness of Prompt</em></figcaption>
                </figure>
              </div>
              <div class="carousel-item">
                <figure class="image">
                  <img src="assets/tables/countinous_vs_discrete.png" alt="Continuous vs Discrete Maps">
                  <figcaption class="has-text-centered"><em>Figure 3: Continuous vs Discrete Maps</em></figcaption>
                </figure>
              </div>
              <div class="carousel-item">
                <figure class="image">
                  <img src="assets/tables/with_vs_without.png" alt="With vs. Without Annotations">
                  <figcaption class="has-text-centered"><em>Figure 4: With vs. Without Annotations</em></figcaption>
                </figure>
              </div>
              <div class="carousel-item">
                <figure class="image">
                  <img src="assets/tables/countre_wise.png" alt="Country wise performance">
                  <figcaption class="has-text-centered"><em>Figure 5: Country wise performance</em></figcaption>
                </figure>
              </div>

              <div class="carousel-item">
                <figure class="image">
                  <img src="assets/tables/human_vs_model.png" alt="Question-Answer Types">
                  <figcaption class="has-text-centered"><em>Figure 6: Question-Answer Types</em></figcaption>
                </figure>
              </div>
            </div>
          
            <!-- Navigation Controls -->
            <button class="carousel-control prev" onclick="moveSlide(-1)">&#10094;</button>
            <button class="carousel-control next" onclick="moveSlide(1)">&#10095;</button>
          </div>

          <script>
            let currentSlide = 0;
            const slides = document.querySelectorAll(".carousel-item");
            
            function moveSlide(direction) {
              slides[currentSlide].classList.remove("active");
              currentSlide = (currentSlide + direction + slides.length) % slides.length;
              slides[currentSlide].classList.add("active");
            }
          </script>

          <h3 class="title is-4 has-text-left">Counterfactual Experiments</h3>
          <div class="content has-text-justified">
            <p>
              To analyze how models rely on internal knowledge versus actual map interpretation, we designed three types
              of counterfactual datasets. These datasets forced models to focus exclusively on the provided maps by
              altering the textual or numerical elements. For Results of Additional Model, Please visit Appendix in <a href="https://arxiv.org/pdf/2409.00255">Paper</a>
            </p>

            <ul>
              <li><strong>Imaginary Names:</strong> Replaced state names with fictional ones.</li>
              <li><strong>Shuffled Names:</strong> Randomized state names while keeping values unchanged.</li>
              <li><strong>Jumbled Values:</strong> Swapped numerical values among states, legend unchanged.</li>
            </ul>
          </div>

          <div class="columns is-centered">
            <figure class="image">
              <img src="assets/tables/counterfactual.png" alt="Performance on Counterfactual">
              <figcaption class="has-text-centered"><em>Performance on Counterfactual</em></figcaption>

            </figure>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- Include CSS for Carousel -->
  <style>
    .carousel-container {
      display: flex;
      overflow-x: auto;
      scroll-snap-type: x mandatory;
      gap: 10px;
    }

    .carousel-item {
      min-width: 100%;
      scroll-snap-align: center;
    }
  </style>

  <!-- Human Baseline Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Human Baseline</h2>

          <!-- Human Evaluation Process -->

          <div class="content has-text-justified">
            <h3 class="title is-4 has-text-left">Human Evaluation Process</h3>
            <p>
              To establish a baseline, we conducted a human evaluation of the MAPWise dataset. The evaluation included
              <strong>450 randomly selected questions</strong> spanning three countries: USA, India, and China. These
              questions were answered by <strong>three independent human annotators</strong> with expertise in map
              interpretation and spatial reasoning. The annotators followed a <strong>majority voting system</strong> to
              determine the final answers. The dataset included both discrete and continuous maps, covering various
              answer types such as binary, list, ranking, and count-based questions.
            </p>


            <div class="columns is-centered">
              <figure class="image">
                <img src="assets/tables/human_baseline.png" alt="human baseline">
                <figcaption class="has-text-centered"><em>Human Baseline</em></figcaption>
              </figure>
            </div>

            <div class="columns is-centered">
              <figure class="image">
                <img src="assets/tables/majority_voting.png" alt="majority voting">
                <figcaption class="has-text-centered"><em>Majority Voting</em></figcaption>
              </figure>
            </div>

            <!-- Comparison: Human vs. Model Performance -->

            <div class="content has-text-justified">
              <h3 class="title is-4 has-text-left">Comparison: Human vs. Model Performance</h3>
              <p>
                Human evaluators consistently outperformed all Vision-Language Models (VLMs), particularly in
                <strong>reasoning-heavy tasks</strong> such as ranking and list-based questions. While human accuracy for
                binary and direct extraction tasks was near-perfect (above <strong>95%</strong>), models struggled
                significantly, with even the best-performing model (<strong>GPT-4o</strong>) achieving only
                <strong>71.52%</strong> binary accuracy and much lower recall on list-based questions.
              </p>
              <p>
                This stark contrast underscores the challenges faced by VLMs in tasks requiring <strong>advanced spatial
                  and numerical reasoning</strong>. Despite their sophisticated architectures, models exhibited biases,
                hallucinations, and errors when extracting values from the legend. The significant performance gap between
                humans and models highlights the need for advancements in <strong>multimodal learning techniques</strong>
                and improved reasoning strategies to bridge this divide.
              </p>
            </div>

            <div class="columns is-centered">
              <figure class="image">
                <img src="assets/tables/human_vs_model.png" alt="Human vs Model Performance">
                <figcaption class="has-text-centered"><em>Human vs Model Performance</em></figcaption>
  
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Acknowledgment</h2>
          <div class="content has-text-justified">
          <p>
            We are grateful to Arqam Patel, Nirupama Ratna, and Jay Gala for their help with the creation of the MAPWise
            dataset. Their early efforts were instrumental in guiding the development of this research. We also thank
            Adnan Qidwai and Jennifer Sheffield for their valuable insights, which helped improve our work. We extend
            our sincere thanks to the reviewers for their insightful comments and suggestions, which have greatly
            enhanced the quality of this manuscript.
          </p>
          
          <p>
            This research was partially supported by ONR Contract N00014-23-1-2364, and sponsored by the Army Research
            Office under Grant Number W911NF-20-1-0080. The views and conclusions contained in this document are those
            of the authors and should not be interpreted as representing the official policies, either expressed or
            implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce
            and distribute reprints for Government purposes notwithstanding any copyright notation herein.
          </p>
        </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Authors</h2>
          <p style="text-align: justify;"> The MAPWise dataset is prepared by the following people: </p>
			<figure style="margin-left: 0;">
				<img src="assets/photo/srijam.jpeg" style="width:16%;">
				<img src="assets/photo/abhishekr.jpg" style="width:12%;">
				<img src="assets/photo/preranak.png" style="width:15%;">
				<img src="assets/photo/manishs.jpeg" style="width:16%;">
				<img src="assets/photo/danr.jpeg" style="width:14%;">
				<img src="assets/photo/vivekg.jpeg" style="width:16%;">
				<figcaption>From left to right, <a href="https://sri-ja.github.io/">Srija Mukhopadhyay</a>, <a href="https://abhishekrajgaria.github.io/">Abhishek Rajgaria</a>, <a href="">Prerana Khatiwada</a>, <a href="https://manshri.in/">Manish Shrivastava</a>, <a href="https://www.cis.upenn.edu/~danroth/">Dan Roth</a> and <a href="https://vgupta123.github.io">Vivek Gupta</a>. </figcaption>
			</figure>
        </div>
      </div>
    </div>
  </section>
			




  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{mukhopadhyay2024mapwiseevaluatingvisionlanguagemodels,
      title={MAPWise: Evaluating Vision-Language Models for Advanced Map Queries}, 
      author={Srija Mukhopadhyay and Abhishek Rajgaria and Prerana Khatiwada and Vivek Gupta and Dan Roth},
      year={2024},
      eprint={2409.00255},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.00255}, 
}</code></pre>
</div></div>
    </div>
  </section>


  <footer class="footer">
    <div class="container">


      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              <a href="https://map-wise.github.io/">MAPWise</a> website was generated by <a
                href="https://pages.github.com/">Github Pages</a> using source code from <a
                href="https://github.com/nerfies/nerfies.github.io"> nerfies</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>