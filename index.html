<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Providing a New Benchmark for evaluating Vision-Language Models for Advanced Map Queries">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MAPWise: New BenchMark for Advanced Map based Queries</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero custom-hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MAPWise: Evaluating Vision-Language Models for Advanced Map Queries
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="srija.mukhopadhyay@research.iiit.ac.in">Srija Mukhopadhyay</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://abhishekrajgaria.github.io/">Abhishek Rajgaria</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://preranakh.github.io/">Prerana Khatiwada</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.iiit.ac.in/people/faculty/m.shrivastava/">Manish Shrivastava</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cis.upenn.edu/~danroth/">Dan Roth</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://vgupta123.github.io/">Vivek Gupta</a><sup>5</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>IIIT Hyderabad,</span>
              <span class="author-block"><sup>2</sup>University of Utah,</span>
              <span class="author-block"><sup>3</sup>University of Delaware,</span>
              <span class="author-block"><sup>4</sup>University of Pennsylvania,</span>
              <span class="author-block"><sup>5</sup>Arizona State University,</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2409.00255" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2409.00255" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. Will be edited with new Repo Link -->
                <span class="link-block">
                  <a href="https://github.com/abhishekrajgaria/Mapwise"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/abhishekrajgaria/Mapwise/tree/main/dataset"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-map"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- Explore Dataset-->
                <span class="link-block">
                  <a href="explore.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-eye"></i>
                    </span>
                    <span>Explore</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- About Section -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">About</h2>
          <div class="content has-text-justified">
            <p>
              <strong>MAPWise: Evaluating Vision-Language Models for Advanced Map Queries</strong>
            </p>
            <p>
              MAPWise is a novel benchmark designed to evaluate the performance of Vision-Language Models (VLMs) in
              answering map-based questions. Choropleth maps, commonly used for geographical data representation, pose
              unique challenges for VLMs due to their reliance on color variations to encode information. Our study
              introduces a dataset featuring maps from the United States, India, and China, with 1000 manually created
              question-answer pairs per region.
              The dataset includes a diverse set of question templates to test spatial reasoning, numerical
              comprehension, and pattern recognition. Our goal is to push the boundaries of VLM capabilities in handling
              complex map-based reasoning tasks.
            </p>
            <p>
              <strong>TL;DR:</strong> MAPWise is a benchmark designed to evaluate Vision-Language Models (VLMs) on
              map-based questions. It includes 3,000 manually curated Q&A pairs across maps from the United States,
              India, and China, testing spatial reasoning, numerical comprehension, and pattern recognition.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Dataset Creation Procedure Section -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">MAPWise Dataset Creation Procedure</h2>

          <!-- Data Sources -->
          <h3 class="title is-4 has-text-left">Data Sources</h3>
          <div class="columns">
            <div class="column">
              <article class="media box">

                <div class="media-content">
                  <p><strong>India</strong></p>
                  <p>Data from the Reserve Bank of India's <em>Handbook of Statistics on Indian States</em>.</p>
                </div>
              </article>
            </div>
            <div class="column">
              <article class="media box">

                <div class="media-content">
                  <p><strong>USA</strong></p>
                  <p>Healthcare statistics from the Kaiser Family Foundation.</p>
                </div>
              </article>
            </div>
            <div class="column">
              <article class="media box">

                <div class="media-content">
                  <p><strong>China</strong></p>
                  <p>Socioeconomic data from the National Bureau of Statistics of China.</p>
                </div>
              </article>
            </div>
          </div>

          <!-- Map Variations -->
          <h3 class="title is-4 has-text-left">Map Variations</h3>
          <div class="columns">
            <div class="column">
              <div class="box has-text-centered">
                <h4 class="title is-5">Discrete Maps</h4>
                <p>Defined categorical legend with clear boundaries.</p>
              </div>
            </div>
            <div class="column">
              <div class="box has-text-centered">
                <h4 class="title is-5">Continuous Maps</h4>
                <p>Gradual spectrum-based legends.</p>
              </div>
            </div>
            <div class="column">
              <div class="box has-text-centered">
                <h4 class="title is-5">Annotated Maps</h4>
                <p>Maps with labels to aid interpretation.</p>
              </div>
            </div>
            <div class="column">
              <div class="box has-text-centered">
                <h4 class="title is-5">Hatched Maps</h4>
                <p>Alternative representations using texture.</p>
              </div>
            </div>
          </div>


          <!-- Question Generation & Validation Process Section -->
          <h3 class="title is-4 has-text-left">Question Generation & Validation</h3>
          <div class="content has-text-justified">
            <p>
              <strong></strong>To design a Robust Benchmark for Map-Based QA</strong>, we developed question templates
              that vary in difficulty, from simple <em>yes/no</em> questions to complex <em>region association</em>
              tasks requiring spatial reasoning. The dataset includes three primary question types:
              <strong>Binary questions</strong> that require a yes or no answer based on map features,
              <strong>Direct Value Extraction</strong> tasks that involve retrieving specific numerical or categorical
              values from maps, and
              <strong>Region Association</strong> questions that challenge models to identify or count regions meeting
              particular criteria, often requiring geospatial reasoning.
            </p>
            <p>
              To ensure high dataset quality, expert annotators meticulously reviewed all questions and answers. A
              rigorous benchmarking process, including human evaluation, was conducted to assess dataset reliability.
              This evaluation highlights the limitations of existing Vision-Language Models (VLMs) in handling complex
              map-based reasoning tasks and provides valuable insights for future improvements.
            </p>
          </div>



          <!-- Table: Example Questions and Answer Types -->
          <h3 class="title is-4 has-text-left">Example Questions and Answer Types</h3>
          <div class="table-container">
            <table class="table is-striped is-fullwidth">
              <thead>
                <tr>
                  <th>Answer Type</th>
                  <th>Example Question</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Binary</td>
                  <td>Yes or no: California is an outlier compared to its neighbors?</td>
                </tr>
                <tr>
                  <td>Single Word</td>
                  <td>Name the eastern-most state that belongs to a higher value range compared to all its neighbors.
                  </td>
                </tr>
                <tr>
                  <td>List</td>
                  <td>Which states in the East China Sea region have a value higher than state Guangdong?</td>
                </tr>
                <tr>
                  <td>Range</td>
                  <td>What is the least value range in the west coast region?</td>
                </tr>
                <tr>
                  <td>Count</td>
                  <td>How many states bordering Canada have a value lower than New Mexico?</td>
                </tr>
                <tr>
                  <td>Ranking</td>
                  <td>Rank Rajasthan, Gujarat, and Jammu & Kashmir in terms of the legend value in the region bordering
                    Pakistan.</td>
                </tr>
              </tbody>
            </table>
          </div>

          <h3 class="title is-4 has-text-left">Dataset Statistics</h3>
          <div class="columns is-centered">
            <div class="column is-half">
              <figure class="image">
                <img src="assets/tables/map_wise_data_stats.png" alt="MAPWise Data Statistics">
                <figcaption class="has-text-centered"><em>MAPWise Dataset statistic</em></figcaption>
              </figure>
            </div>
          </div>





        </div>
      </div>
    </div>
  </section>

  <!-- Human Baseline Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Human Baseline</h2>

          <!-- Human Evaluation Process -->

          <div class="content has-text-justified">
            <p>
              <strong>Human Evaluation Process</strong>
            </p>
            <p>
              To establish a baseline, we conducted a human evaluation of the MAPWise dataset. The evaluation included
              <strong>450 randomly selected questions</strong> spanning three countries: USA, India, and China. These
              questions were answered by <strong>three independent human annotators</strong> with expertise in map
              interpretation and spatial reasoning. The annotators followed a <strong>majority voting system</strong> to
              determine the final answers. The dataset included both discrete and continuous maps, covering various
              answer types such as binary, list, ranking, and count-based questions.
            </p>


            <div class="columns is-centered">
              <figure class="image">
                <img src="assets/tables/human_baseline.png" alt="human baseline">
                <figcaption class="has-text-centered"><em>Human Baseline</em></figcaption>
              </figure>
            </div>

            <div class="columns is-centered">
              <figure class="image">
                <img src="assets/tables/majority_voting.png" alt="majority voting">
                <figcaption class="has-text-centered"><em>Majority Voting</em></figcaption>
              </figure>
            </div>

          </div>



          <!-- Comparison: Human vs. Model Performance -->

          <div class="content has-text-justified">
            <p>
              <strong>Comparison: Human vs. Model Performance</strong>
            </p>
            <p>
              Human evaluators consistently outperformed all Vision-Language Models (VLMs), particularly in
              <strong>reasoning-heavy tasks</strong> such as ranking and list-based questions. While human accuracy for
              binary and direct extraction tasks was near-perfect (above <strong>95%</strong>), models struggled
              significantly, with even the best-performing model (<strong>GPT-4o</strong>) achieving only
              <strong>71.52%</strong> binary accuracy and much lower recall on list-based questions.
            </p>
            <p>
              This stark contrast underscores the challenges faced by VLMs in tasks requiring <strong>advanced spatial
                and numerical reasoning</strong>. Despite their sophisticated architectures, models exhibited biases,
              hallucinations, and errors when extracting values from the legend. The significant performance gap between
              humans and models highlights the need for advancements in <strong>multimodal learning techniques</strong>
              and improved reasoning strategies to bridge this divide.
            </p>



          </div>
          <div class="columns is-centered">
            <figure class="image">
              <img src="assets/tables/human_vs_model.png" alt="Human vs Model Performance">
              <figcaption class="has-text-centered"><em>Human vs Model Performance</em></figcaption>

            </figure>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- Results & Analysis Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Results & Analysis</h2>

          <!-- Models Used -->
          <h3 class="title is-4 has-text-left">Models Evaluated</h3>
          <div class="content has-text-justified">
            <p>
              We evaluated multiple Vision-Language Models (VLMs) on the <strong>MAPWise</strong> dataset. The models
              included both <strong>closed-source</strong> and <strong>open-source</strong> multimodal models:
            </p>
            <ul>
              <li><strong>Closed-Source Models:</strong> GPT-4o, Gemini 1.5 Flash</li>
              <li><strong>Open-Source Models:</strong> CogAgent, InternLM-XComposer2, Idefics 2, Qwen VL</li>
            </ul>

            <p>
              Further analysis was conducted to evaluate <strong>closed-source model performance</strong>, the
              <strong>effectiveness of different prompting strategies</strong>, and <strong>biases in model
                predictions</strong>. The study examined how models performed across different map types
              (<strong>discrete vs. continuous</strong>, <strong>colored vs. hatched</strong>), annotations
              (<strong>with vs. without labels</strong>), and <strong>country-wise variations</strong>. Additionally,
              model accuracy was compared across various <strong>question-answer types</strong>, revealing significant
              gaps in spatial and numerical reasoning. For Complete Results, Please visit Appendix in <a href="https://arxiv.org/pdf/2409.00255">Paper</a>
            </p>
          </div>

          <!-- Carousel for Results -->
          <h3 class="title is-4 has-text-left">Performance Visualizations</h3>
          <div class="carousel">
            <div class="carousel-container">
              <div class="carousel-item">
                <figure class="image">
                  <img src="path/to/image1.png" alt="Analysis Image 1">
                  <figcaption class="has-text-centered"><em>Figure 1: Model Accuracy Comparison</em></figcaption>
                </figure>
              </div>
              <div class="carousel-item">
                <figure class="image">
                  <img src="path/to/image2.png" alt="Analysis Image 2">
                  <figcaption class="has-text-centered"><em>Figure 2: Precision vs Recall</em></figcaption>
                </figure>
              </div>
              <div class="carousel-item">
                <figure class="image">
                  <img src="path/to/image3.png" alt="Analysis Image 3">
                  <figcaption class="has-text-centered"><em>Figure 3: Question Type Breakdown</em></figcaption>
                </figure>
              </div>
              <div class="carousel-item">
                <figure class="image">
                  <img src="path/to/image4.png" alt="Analysis Image 4">
                  <figcaption class="has-text-centered"><em>Figure 4: Error Analysis</em></figcaption>
                </figure>
              </div>
              <div class="carousel-item">
                <figure class="image">
                  <img src="path/to/image5.png" alt="Analysis Image 5">
                  <figcaption class="has-text-centered"><em>Figure 5: Performance across Regions</em></figcaption>
                </figure>
              </div>
            </div>
          </div>


          <h3 class="title is-4 has-text-left">Counterfactual Experiments</h3>
          <div class="content has-text-justified">
            <p>
              To analyze how models rely on internal knowledge versus actual map interpretation, we designed three types
              of counterfactual datasets. These datasets forced models to focus exclusively on the provided maps by
              altering the textual or numerical elements. For Results of Additional Model, Please visit Appendix in <a href="https://arxiv.org/pdf/2409.00255">Paper</a>
            </p>

            <ul>
              <li><strong>Imaginary Names:</strong> Replaced state names with fictional ones.</li>
              <li><strong>Shuffled Names:</strong> Randomized state names while keeping values unchanged.</li>
              <li><strong>Jumbled Values:</strong> Swapped numerical values among states, legend unchanged.</li>
            </ul>
          </div>

          <div class="columns is-centered">
            <figure class="image">
              <img src="assets/tables/counterfactual.png" alt="Performance on Counterfactual">
              <figcaption class="has-text-centered"><em>Performance on Counterfactual</em></figcaption>

            </figure>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- Include CSS for Carousel -->
  <style>
    .carousel-container {
      display: flex;
      overflow-x: auto;
      scroll-snap-type: x mandatory;
      gap: 10px;
    }

    .carousel-item {
      min-width: 100%;
      scroll-snap-align: center;
    }
  </style>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-left">Acknowledgment</h2>
          <div class="content has-text-justified">
          <p>
            We are grateful to Arqam Patel, Nirupama Ratna, and Jay Gala for their help with the creation of the MAPWise
            dataset. Their early efforts were instrumental in guiding the development of this research. We also thank
            Adnan Qidwai and Jennifer Sheffield for their valuable insights, which helped improve our work. We extend
            our sincere thanks to the reviewers for their insightful comments and suggestions, which have greatly
            enhanced the quality of this manuscript.
          </p>
          
          <p>
            This research was partially supported by ONR Contract N00014-23-1-2364, and sponsored by the Army Research
            Office under Grant Number W911NF-20-1-0080. The views and conclusions contained in this document are those
            of the authors and should not be interpreted as representing the official policies, either expressed or
            implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce
            and distribute reprints for Government purposes notwithstanding any copyright notation herein.
          </p>
        </div>
        </div>
      </div>
    </div>
  </section>




  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{mukhopadhyay2024mapwiseevaluatingvisionlanguagemodels,
      title={MAPWise: Evaluating Vision-Language Models for Advanced Map Queries}, 
      author={Srija Mukhopadhyay and Abhishek Rajgaria and Prerana Khatiwada and Vivek Gupta and Dan Roth},
      year={2024},
      eprint={2409.00255},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.00255}, 
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">


      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              <a href="https://map-wise.github.io/">MAPWise</a> website was generated by <a
                href="https://pages.github.com/">Github Pages</a> using source code from <a
                href="https://github.com/nerfies/nerfies.github.io"> nerfies</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>